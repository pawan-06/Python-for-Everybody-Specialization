{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mu1.png)\n",
    "**PAWAN KUMAR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python to Access Web Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**by University of Michigan**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEEK_1 Getting Started\n",
    "Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing and Running Python Screen Shots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEEK_2 Regular Expressions (Chapter 11)\n",
    "Assignment_11 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting Data With Regular Expressions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding Numbers in a Haystack**\n",
    "\n",
    "In this assignment you will read through and parse a file with text and numbers. You will extract all the numbers in the file and compute the sum of the numbers.\n",
    "\n",
    "Data Files\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/regex_sum_42.txt (There are 90 values with a sum=445833)\n",
    "Actual data: http://py4e-data.dr-chuck.net/regex_sum_1146200.txt (There are 73 values and the sum ends with 168)\n",
    "These links open in a new window. Make sure to save the file into the same folder as you will be writing your Python program. Note: Each student will have a distinct data file for the assignment - so only use your own data file for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling The Data**\n",
    "The basic outline of this problem is to read the file, look for integers using the re.findall(), looking for a regular expression of '[0-9]+' and then converting the extracted strings to integers and summing up the integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter file:regex_sum_1146200.txt\n",
      "309168\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "name = input(\"Enter file:\")\n",
    "if len(name) < 1 : name = \"regex_sum_1146200.txt\"\n",
    "fh = open(name)\n",
    "newlist = list()\n",
    "for line in fh :\n",
    "    line = re.findall('[0-9]+', line)  \n",
    "    for number in line :\n",
    "        newlist.append(int(number)) \n",
    "print(sum(newlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEEK_3 Networks and Sockets (Chapter 12)\n",
    "Assignment_12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring the HyperText Transport Protocol**\n",
    "\n",
    "You are to retrieve the following document using the HTTP protocol in a way that you can examine the HTTP Response headers.\n",
    "\n",
    "http://data.pr4e.org/intro-short.txt\n",
    "There are three ways that you might retrieve this web page and look at the response headers:\n",
    "\n",
    "Preferred: Modify the socket1.py program to retrieve the above URL and print out the headers and data. Make sure to change the code to retrieve the above URL - the values are different for each URL.\n",
    "Open the URL in a web browser with a developer console or FireBug and manually examine the headers that are returned.\n",
    "Enter the header values in each of the fields below and press \"Submit\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given socket1.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(),end='')\n",
    "\n",
    "mysock.close()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify socket1.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r\n",
      "Date: Fri, 05 Mar 2021 10:00:59 GMT\r\n",
      "Server: Apache/2.4.18 (Ubuntu)\r\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\r\n",
      "ETag: \"1d3-54f6609240717\"\r\n",
      "Accept-Ranges: bytes\r\n",
      "Content-Length: 467\r\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\r\n",
      "Pragma: no-cache\r\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\r\n",
      "Connection: close\r\n",
      "Content-Type: text/plain\r\n",
      "\r\n",
      "Why should you learn to write programs?\n",
      "\n",
      "Writing programs (or programming) is a very creative \n",
      "and rewarding activity.  You can write programs for \n",
      "many reasons, ranging from making your living to solving\n",
      "a difficult data analysis problem to having fun to helping\n",
      "someone else solve a problem.  This book assumes that \n",
      "everyone needs to know how to program, and that once \n",
      "you know how to program you will figure out what you want \n",
      "to do with your newfound skills.  \n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(),end='')\n",
    "\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEEK_4 Programs that Surf the Web (Chapter 12)\n",
    "Assignment_12.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping Numbers from HTML using BeautifulSoup** In this assignment you will write a Python program similar to http://www.py4e.com/code3/urllink2.py. The program will use urllib to read the HTML from the data files below, and parse the data, extracting numbers and compute the sum of the numbers in the file.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553)\n",
    "Actual data: http://py4e-data.dr-chuck.net/comments_1146202.html (Sum ends with 82)\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    print('TAG:', tag)\n",
    "    print('URL:', tag.get('href', None))\n",
    "    print('Contents:', tag.contents[0])\n",
    "    print('Attrs:', tag.attrs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Enter - http://py4e-data.dr-chuck.net/comments_42.html\n",
    "Count 50\n",
    "Sum 2..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modyfy code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://py4e-data.dr-chuck.net/comments_1146202.html\n",
      "count 50\n",
      "sum 2582\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# url: http://py4e-data.dr-chuck.net/comments_1146202.html\n",
    "url = input('Enter - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "spans = soup(\"span\")\n",
    "numbers = []\n",
    "count = 0\n",
    "for span in spans:\n",
    "    numbers.append(int(span.string))\n",
    "    count += 1\n",
    "print(\"count\",count)\n",
    "print(\"sum\",sum(numbers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment_12.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Following Links in HTML Using BeautifulSoup**\n",
    "\n",
    "In this assignment you will write a Python program that expands on http://www.py4e.com/code3/urllinks.py. The program will use urllib to read the HTML from the data files below, extract the href= vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name in the list, follow that link and repeat the process a number of times and report the last name you find.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the name for your testing and the other is the actual data you need to process for the assignment\n",
    "\n",
    "Sample problem: Start at http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "Find the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. The answer is the last name that you retrieve.\n",
    "Sequence of names: Fikret Montgomery Mhairade Butchi Anayah\n",
    "Last name in sequence: Anayah\n",
    "Actual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Maryjane.html\n",
    "Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.\n",
    "Hint: The first character of the name of the last page that you will load is: E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given_excution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Enter URL: http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "Enter count: 4\n",
    "Enter position: 3\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Montgomery.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Mhairade.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Butchi.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Anayah.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modyfy_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter URL: http://py4e-data.dr-chuck.net/known_by_Maryjane.html\n",
      "Enter count: 7\n",
      "Enter position: 18\n",
      "retrieving: http://py4e-data.dr-chuck.net/known_by_Maryjane.html\n",
      "retrieving: http://py4e-data.dr-chuck.net/known_by_Nawal.html\n",
      "retrieving: http://py4e-data.dr-chuck.net/known_by_Alfred.html\n",
      "retrieving: http://py4e-data.dr-chuck.net/known_by_Sineidin.html\n",
      "retrieving: http://py4e-data.dr-chuck.net/known_by_Neeve.html\n",
      "retrieving: http://py4e-data.dr-chuck.net/known_by_Taddy.html\n",
      "retrieving: http://py4e-data.dr-chuck.net/known_by_Alexis.html\n",
      "Elysa\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# url is http://py4e-data.dr-chuck.net/known_by_Maryjane.html\n",
    "url = input(\"Enter URL: \")\n",
    "count = int(input(\"Enter count: \"))\n",
    "position = int(input(\"Enter position: \"))\n",
    "\n",
    "\n",
    "names = []\n",
    "\n",
    "while count > 0:\n",
    "    print (\"retrieving: {0}\".format(url))\n",
    "    html = urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    anchors = soup('a')\n",
    "    name = anchors[position-1].string\n",
    "    names.append(name)\n",
    "    url = anchors[position-1]['href']\n",
    "    count -= 1\n",
    "\n",
    "print (names[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEEK_5 Web Services and XML (Chapter 13)\n",
    "Assignment_13.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting Data from XML**\n",
    "\n",
    "In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/geoxml.py. The program will prompt for a URL, read the XML data from that URL using urllib and then parse and extract the comment counts from the XML data, compute the sum of the numbers in the file.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/comments_42.xml (Sum=2553)\n",
    "Actual data: http://py4e-data.dr-chuck.net/comments_1146204.xml (Sum ends with 45)\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import urllib.request, urllib.parse, urllib.error\n",
    "import xml.etree.ElementTree as ET\n",
    "import ssl\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "# https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/xml?'\n",
    "else :\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/xml?'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "\n",
    "    data = uh.read()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "    print(data.decode())\n",
    "    tree = ET.fromstring(data)\n",
    "\n",
    "    results = tree.findall('result')\n",
    "    lat = results[0].find('geometry').find('location').find('lat').text\n",
    "    lng = results[0].find('geometry').find('location').find('lng').text\n",
    "    location = results[0].find('formatted_address').text\n",
    "\n",
    "    print('lat', lat, 'lng', lng)\n",
    "    print(location)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given_excution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Enter location: http://py4e-data.dr-chuck.net/comments_42.xml\n",
    "Retrieving http://py4e-data.dr-chuck.net/comments_42.xml\n",
    "Retrieved 4189 characters\n",
    "Count: 50\n",
    "Sum: 2..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter location: http://py4e-data.dr-chuck.net/comments_1146204.xml\n",
      "Retrieving http://py4e-data.dr-chuck.net/comments_1146204.xml\n",
      "Retrieved 4229 characters\n",
      "Count: 50\n",
      "Sum:2545\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "serviceurl = 'http://maps.googleapis.com/maps/api/geocode/xml?'\n",
    "\n",
    "url = input('Enter location: ')\n",
    "\n",
    "print('Retrieving', url)\n",
    "uh = urllib.request.urlopen(url)\n",
    "data = uh.read()\n",
    "print('Retrieved', len(data), 'characters')\n",
    "#print(data.decode())\n",
    "tree = ET.fromstring(data)\n",
    "\n",
    "counts =  tree.findall('.//count')\n",
    "print (\"Count: \" + str(len(counts)))\n",
    "\n",
    "accumulator = 0\n",
    "\n",
    "for count in counts:\n",
    "    accumulator += int(count.text)\n",
    "\n",
    "print (\"Sum:\" + str(accumulator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEEK_6 JSON and the REST Architecture (Chapter 13)\n",
    "Assignment_13.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting Data from JSON**\n",
    "\n",
    "In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/json2.py. The program will prompt for a URL, read the JSON data from that URL using urllib and then parse and extract the comment counts from the JSON data, compute the sum of the numbers in the file and enter the sum below:\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/comments_42.json (Sum=2553)\n",
    "Actual data: http://py4e-data.dr-chuck.net/comments_1146205.json (Sum ends with 68)\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import json\n",
    "\n",
    "data = '''\n",
    "[\n",
    "  { \"id\" : \"001\",\n",
    "    \"x\" : \"2\",\n",
    "    \"name\" : \"Chuck\"\n",
    "  } ,\n",
    "  { \"id\" : \"009\",\n",
    "    \"x\" : \"7\",\n",
    "    \"name\" : \"Brent\"\n",
    "  }\n",
    "]'''\n",
    "\n",
    "info = json.loads(data)\n",
    "print('User count:', len(info))\n",
    "\n",
    "for item in info:\n",
    "    print('Name', item['name'])\n",
    "    print('Id', item['id'])\n",
    "    print('Attribute', item['x'])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given_excution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Enter location: http://py4e-data.dr-chuck.net/comments_42.json\n",
    "Retrieving http://py4e-data.dr-chuck.net/comments_42.json\n",
    "Retrieved 2733 characters\n",
    "Count: 50\n",
    "Sum: 2..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter location: http://py4e-data.dr-chuck.net/comments_1146205.json\n",
      "Retrieving http://py4e-data.dr-chuck.net/comments_1146205.json\n",
      "Retrieved 2726 characters\n",
      "count:  50\n",
      "sum:  2668\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "url = input('Enter location: ')\n",
    "address = urllib.request.urlopen(url)\n",
    "data = address.read()\n",
    "\n",
    "print('Retrieving', url)\n",
    "print('Retrieved', len(data), 'characters')\n",
    "\n",
    "info = json.loads(data)\n",
    "info = info[\"comments\"]\n",
    "\n",
    "sum = 0\n",
    "count = 0\n",
    "for item in info:\n",
    "    \n",
    "    sum += int(item[\"count\"])\n",
    "    count += 1\n",
    "print(\"count: \",count)\n",
    "print(\"sum: \", sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment_13.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calling a JSON API**\n",
    "\n",
    "In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/geojson.py. The program will prompt for a location, contact a web service and retrieve JSON for the web service and parse that data, and retrieve the first place_id from the JSON. A place ID is a textual identifier that uniquely identifies a place as within Google Maps.\n",
    "**API End Points**\n",
    "\n",
    "To complete this assignment, you should use this API endpoint that has a static subset of the Google Data:\n",
    "\n",
    "http://py4e-data.dr-chuck.net/json?\n",
    "This API uses the same parameter (address) as the Google API. This API also has no rate limit so you can test as often as you like. If you visit the URL with no parameters, you get \"No address...\" response.\n",
    "To call the API, you need to include a key= parameter and provide the address that you are requesting as the address= parameter that is properly URL encoded using the urllib.parse.urlencode() function as shown in http://www.py4e.com/code3/geojson.py\n",
    "\n",
    "Make sure to check that your code is using the API endpoint is as shown above. You will get different results from the geojson and json endpoints so make sure you are using the same end point as this autograder is using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "# https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "else :\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "\n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        continue\n",
    "\n",
    "    print(json.dumps(js, indent=4))\n",
    "\n",
    "    lat = js['results'][0]['geometry']['location']['lat']\n",
    "    lng = js['results'][0]['geometry']['location']['lng']\n",
    "    print('lat', lat, 'lng', lng)\n",
    "    location = js['results'][0]['formatted_address']\n",
    "    print(location)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Execution**\n",
    "\n",
    "You can test to see if your program is working with a location of \"South Federal University\" which will have a place_id of \"ChIJ1Z9sheJZkFQRDePQqQebCdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Enter location: South Federal University\n",
    "Retrieving http://...\n",
    "Retrieved 2275 characters\n",
    "Place id ChIJ1Z9sheJZkFQRDePQqQebCdg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modyfy_code**\n",
    "Please run your program to find the place_id for this location:\n",
    "\n",
    "Spiru Haret University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter location: Spiru Haret University\n",
      "Retrieving http://py4e-data.dr-chuck.net/json?address=Spiru+Haret+University&key=42\n",
      "Retrieved 2325 characters\n",
      "place_id: ChIJfUpIYT__sUARWffJ-6LAxO8\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "# https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "else :\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "\n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        continue\n",
    "\n",
    "    placeid = js['results'][0]['place_id']\n",
    "    print('place_id:',placeid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check my Accomplishment**\n",
    "link:-  https://www.coursera.org/account/accomplishments/verify/C96BVEWTEP65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This course will show how one can treat the Internet as a source of data.  We will scrape, parse, and read web data as well as access data using web APIs.  We will work with HTML, XML, and JSON data formats in Python.  This course will cover Chapters 11-13 of the textbook “Python for Everybody”. To succeed in this course, you should be familiar with the material covered in Chapters 1-10 of the textbook and the first two courses in this specialization.  These topics include variables and expressions, conditional execution (loops, branching, and try/except), functions, Python data structures (strings, lists, dictionaries, and tuples), and manipulating files.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
